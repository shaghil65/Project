"""Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HnrSTUTv0laCo2N69FnuOnZx4gwhjZ_s
"""

#Mounting Our Drive In The Colab
from google.colab import drive
drive.mount('/content/drive')
#Importing Pandas Library 
import pandas as pd
import numpy as np
#Importing train_test_split For Spliting Our Train Data To 80-20%
from sklearn.model_selection import train_test_split
#Importing VarianceThreshold  From Features Selection
from sklearn.feature_selection import VarianceThreshold
#Importing MultiClass Classifier DecisionTreeClassifier From SkLearn
from sklearn.tree import DecisionTreeClassifier
#Importing MultiClass Classifier BaggingClassifier From SkLearn
from sklearn.ensemble import BaggingClassifier
#Importing MultiClass Classifier ExtraTreeClassifier From SkLearn
from sklearn.tree import ExtraTreeClassifier
#Importing MultiClass Classifier RidgeClassifier From SkLearn
from sklearn.linear_model import RidgeClassifier
#Importing MultiClass Classifier RandomForestClassifier From SkLearn
from sklearn.ensemble import RandomForestClassifier
#Loading Training Data From The Drive
train=pd.read_csv('/content/drive/MyDrive/smtp/train.csv')
#Loading Test Data From The Drive
test=pd.read_csv('/content/drive/MyDrive/smtp/test.csv')

#Dropping The Columns With NAN Values From Training Data
train = train.drop([ 'Soil_Type7', 'Soil_Type15','Soil_Type1'], axis=1)
#Dropping The Columns With NAN Values From Test Data
test= test.drop(['Soil_Type7', 'Soil_Type15','Soil_Type1'], axis=1)
#Remove Low Variance Columns
selector = VarianceThreshold()
selector.fit_transform(train)
#Copying The Train Data Into Normalized Training Data
train_norm = train.copy()

#Function For Normalizing The Data
def normalize(df):
  #First Making The Copy Of The Datadrame
    result = df.copy()
    #Looping Through The DataFrame
    for feature_name in df.columns:
      #Selecting The Maximum Value Of The Feature And Storing It Into The max_value
        max_value = df[feature_name].max()
        #Selecting The Maximum Value Of The Feature And Storing It Into The min_value
        min_value = df[feature_name].min()
        #Adding That Value To The Copy Dataframes Cell
        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)
    #Returning The Normalized DataFrame
    return result

#Preparing a Second training set from the train dataframe by normalizing all columns between 0 and 1 Using Normalize Function
print("The Normalize Data",normalize(train_norm))

print(train)

#Firstly Training 3 Models For Train Data(Without Normalization)
#Spliting Our Train Data(Without Normalization) Into 80% And 20%

#For Labels
Labels = train.Cover_Type

#For Features
Features = train.drop('Cover_Type', axis=1)

#Splitting The Data Into 80%(For Training t_train) And 20%(For Testing t_test)
t_train, t_test, y_train, y_test = train_test_split(Features, Labels,test_size=0.2)

#Here We Have Tweaked Our classifiers By  Adjusting The Parameters
dtree_model = DecisionTreeClassifier(criterion="entropy",max_depth = 20000000).fit(t_train, y_train)
#Predicting The Values Of Local Splitted Test Data
Cover_type = dtree_model.predict(t_test)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data 
Tree=dtree_model.score(t_test,y_test)
#Printing The Accuracy Socre Of The Decision Tree Classifier 
print("The Accuracy Score Of Decision Tree Classifier On Local Test Data",Tree*100)

#Here We Have Tweaked Our classifiers By  Adjusting The Parameters
extra_tree = ExtraTreeClassifier(random_state=0,criterion ='entropy')
cls = BaggingClassifier(extra_tree, random_state=0).fit(t_train, y_train)
#Fitting Our training Data
cls = BaggingClassifier(extra_tree, random_state=0).fit(t_train, y_train)
#Predicting The Values Of Local Splitted Test Data
Cover_type = cls.predict(t_test)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data 
ETree=cls.score(t_test,y_test)
#Printing The Accuracy Score Of The Decision Tree Classifier 
print("The Accuracy Score Of Extra Tree Classifier On Local Test Data",ETree*100)

#Here We Are #Fitting Our training Data
clf = RidgeClassifier().fit(t_train, y_train)
#Predicting The Values Of Local Splitted Test Data
Cover_type = clf.predict(t_test)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data 
Ridge=clf.score(t_test,y_test)
#Printing The Accuracy Score Of The Decision Tree Classifier 
print("The Accuracy Score Of RidgeClassifier On Local Test Data",Ridge*100)

#Here We Are #Fitting Our training Data
clf = RandomForestClassifier(max_depth=100, random_state=0)
clf.fit(t_train,y_train)
#Predicting The Values Of Local Splitted Test Data
Cover_type = clf.predict(t_test)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data 
RandomForest=clf.score(t_test,y_test)
#Printing The Accuracy Score Of The Decision Tree Classifier 
print("The Accuracy Score Of RidgeClassifier On Local Test Data",RandomForest*100)

#Firslty Training 3 Models For Train Data(With Normalization)
#Spliting Our Train_Norm Data(With Normalization) Into 80% And 20%

#For Labels
Labels_Norm = train_norm.Cover_Type

#For Features
Features_Norm = train_norm.drop('Cover_Type', axis=1)

#Splitting The Data Into 80%(For Training t_train) And 20%(For Testing t_test)
t_train_Norm, t_test_Norm, y_train_Norm, y_test_Norm = train_test_split(Labels_Norm, Features_Norm,test_size=0.2)

#Here We Have Tweaked Our classifiers By  Adjusting The Parameters
dtree_model = DecisionTreeClassifier(criterion="entropy",max_depth = 200000).fit(t_train_Norm, y_train_Norm)
#Predicting The Values Of Local Splitted Test Data
Cover_type = dtree_model.predict(t_test_Norm)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data
Tree=dtree_model.score(t_test_Norm,y_test_Norm)
#Printing The Accuracy Socre Of The Decision Tree Classifier
print("The Accuracy Score Of Decision Tree Classifier On Local Test Data",Tree*100)

#Here We Have Tweaked Our classifiers By  Adjusting The Parameters
extra_tree = ExtraTreeClassifier(random_state=0,criterion ='entropy')
#Fitting Our training Data
cls = BaggingClassifier(extra_tree, random_state=0).fit(t_train_Norm, y_train_Norm)
#Predicting The Values Of Local Splitted Test Data
Cover_type = cls.predict(t_test_Norm)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data
ETree=cls.score(t_test_Norm,y_test_Norm)
#Printing The Accuracy Score Of The Decision Tree Classifier
print("The Accuracy Score Of Extra Tree Classifier On Local Test Data",ETree*100)

#Here We Are #Fitting Our training Data
clf = RidgeClassifier().fit(t_train_Norm, y_train_Norm)
#Predicting The Values Of Local Splitted Test Data
Cover_type = clf.predict(t_test_Norm)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data
Ridge=clf.score(t_test_Norm,y_test_Norm)
#Printing The Accuracy Score Of The Decision Tree Classifier
print("The Accuracy Score Of RidgeClassifier On Local Test Data",Ridge*100)

#Here We Are #Fitting Our training Data
clf = RandomForestClassifier(max_depth=100, random_state=0)
clf.fit(t_train_Norm,y_train_Norm)
#Predicting The Values Of Local Splitted Test Data
Cover_type = clf.predict(t_test_Norm)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data
RandomForest=clf.score(t_test_Norm,y_test_Norm)
#Printing The Accuracy Score Of The Decision Tree Classifier
print("The Accuracy Score Of RidgeClassifier On Local Test Data",RandomForest*100)

#Here We Have Tweaked Our classifiers By  Adjusting The Parameters
dtree_model = DecisionTreeClassifier(criterion="entropy",max_depth = 20000000).fit(t_train, y_train)
#Predicting The Values Of Local Splitted Test Data
Cover_type = dtree_model.predict(t_test)
#Printing The Predicted Value Of Local Splitted Test Data
print("The Predicted Values Cover Type Of Local Test Data",Cover_type)
#Checking The  accuracy This Model Using The Local Test Data
Tree=dtree_model.score(t_test,y_test)
#Printing The Accuracy Socre Of The Decision Tree Classifier
print("The Accuracy Score Of Decision Tree Classifier On Local Test Data",Tree*100)
#Predicting The Test Data Values
Cover_type=dtree_model.predict(test)
#Printing The Predicting Values
print("The Predicted Values For The Kaggle Test Data Cover Type",Cover_type)
#Exporting The Id And Cover_Type Columns Into Sample Csv
sample = test[['Id']].copy()
sample['Cover_Type'] = Cover_type
print(sample)
#Creating Our Csv File With That Two Exported Columns For Submission On Kaggle
sample.to_csv('sample.csv',index=False)
